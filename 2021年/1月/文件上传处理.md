# 文件上传处理

> 限制同时上传文件数量，是为了更好的体验，而不是一下子上传10个，全部一起请求。
>
> 分片上传，在处理大文件中，显得特别重要，因为服务一下子接受这么大的文件读取非常耗性能。



#### 限制同时上传数量

怎么限制？我看过一些书籍推荐的办法就是promise，promise是最好的解决方案。

其他的一些方案，比如原生加锁，迭代器，等等。

```js
async getFileData(e) {
      this.loadingFlag = true // 不允许提交
      let filesArray = [] // 文件二进制列表
      let viewFile = []  // 页面显示的数据
      for (let i = 0; i < e.target.files.length; i++) {
        let obj = {
          percentage: 0,
        }
        for (const key in e.target.files[i]) {
          obj[key] = e.target.files[i][key]
        }
        obj.size = (obj.size / 1048576).toFixed(2) + ' MB'
        viewFile.push(obj)
        filesArray.push(e.target.files[i])
      }
      this.viewFile = [...this.viewFile, ...viewFile]
			// 重点是这里开始上传了 
      this.limitLoad(filesArray, this.uploadFile)
      e.target.value = '' // 当参数拿到后，清空e.target.value,免得数据显示在页面上面，因为vue检测不到所以组件不会更新的，只能自己去处理，有v-if，等等其他办法
},

 async limitLoad(files, handler, limit = 3) {
      const fileArray = files
      const that = this
      // 初始化,并发请求到最大数，并发limit项
      let promises = fileArray.splice(0, limit).map(async (item, index) => {
        // 这里返回的 index 是任务在 promises 的下标，
        // 用于在 Promise.race 之后找到完成的任务下标
        // 后台需要的文件后缀，它会返回来，文件id，name。
        let suffix = item.name.split('.').pop().toLowerCase()
        let res = await getUploadId({ suffix })
        // handler 将正在处理的任务返回出去
        return handler(item, res, that.getUploadUrl, that.progress).then(() => {
          return index
        })
      })
      (async function loop() {
        // handler 最快处理完的promises
        let p = Promise.race(promises)
        // 遍历剩余数据
        for (let i = 0; i < fileArray.length; i++) {
          // 更新 Promise.race
          p = p.then(async (newIndex) => {
            // 将正在处理的任务返回出去
            let suffix = fileArray[i].name.split('.').pop().toLowerCase()
            let res = await getUploadId({ suffix })
            promises[newIndex] = handler(fileArray[i], res, that.getUploadUrl, that.progress).then(() => {
              return newIndex
            })
            // 重新执行promises
            return Promise.race(promises)
          })
        }
      })() // 自执行函数
      
      // 我想重点解析一下loop 自执行函数，因为关键点就在这里，上面直接跑3个promsie，然后谁回来就会触发promise.race
      // loop里面，先赋值 p = Promise.race(promises)，然后剩余数据全部遍历一遍，then里面是异步，只要遇见谁结束了，先把新的要处理数据处理好，并且返回出去，接着return Promise.race(promises)。这么简单的promise，代码量这么少，难道不香吗？
  },
    
    
    // 剩下的两个回调感觉一样不太大
    
    // 上传进度
    progress(currentChunk, chunks, file) {
      console.log(currentChunk, chunks, file, 'progress')
      let flag = false
      this.viewFile.forEach((item) => {
        if (item.name === file.name) {
          item.percentage = (currentChunk / chunks).toFixed(2) * 100
          console.log(item.percentage, 'item.percentage')
          if (item.percentage != 100) {
            // 没有全部上传完不能提交
            flag = true
          }
        }
      })
      this.loadingFlag = flag
    },
    getUploadUrl(res) {
      this.addDoc.items = [...this.addDoc.items, res]
      this.addDoc.items.forEach((item) => {
        item.file_path = item.name
      })
      console.log(this.addDoc.items, '上传成功')
    },
    
```



#### 分片上传

思路就是将input 传过来的e.target.files 这个数组，里面的每一项，用slice 切割成一片一片的上传，就完成了，当然需要做的事还要一下比较杂的，代码呈现。

`spark-md5:`当他们的计算方式一样的时候，每一个文件都有一个唯一的md5值，这就可以给后台一些判断，md5值对比后发现，这个文件我很久之前上传过了，那我可以返回去前端告诉他，本文件已上传成功。

```js
import Vue from 'vue'
// uploadPart 单片上传接口，uploadComplete上传完成接口，其实还有一个初始化的接口，后面代码会有的
import { uploadPart, uploadComplete } from '@/api/base-api' 
import SparkMD5 from 'spark-md5'

export default function () {
  // 将上传文件的方法挂载到vue的原型链上面
  Vue.prototype.uploadFile = uploadFile
  let spark = new SparkMD5.ArrayBuffer() // 实例化md5
  async function uploadFile(file, params, callback, callbackProgress) {
    // file：文件数据
    // params：上传的参数，就是上面说的初始化的时候传过来的，那么你可以把他优化一下放到里面来也是可以的
    // callback： 上传成功后的回调
    // callbackProgress：上传进度回调 这两个回调可以做一起的，但是考虑的是不同事情尽量分开
    let blobSlice = File.prototype.slice || File.prototype.mozSlice || File.prototype.webkitSlice // 切片方法
    let chunkSize = 1024 * 1024 * 6 // 6M每一块分片，
    let currentChunk = 0 // 当前上传分片是第0块
    let fileReader = new FileReader() // 实例化fileReader
    return new Promise((resolve, reject) => {
      let chunks = Math.ceil(file.size / chunkSize) // 将会切多少块
      // 触发文件第一块上传
      loadNext()
      // 文件切割后的回调，this.result为切割的文件块
      fileReader.onload = async function (e) {
        // md5
        spark.append(e.target.result)
        // 用FormData传输文件对象
        let fd = new FormData()
        fd.append('uploadId', params.uploadId)
        fd.append('name', params.name)
        fd.append('currentPart', currentChunk)
        // 设置上传的当前的文件块
        fd.append('file', new Blob([e.target.result]))
        let res = await uploadPart(fd)
        // 判断当前切片是否小于总切片数量
        if (currentChunk < chunks) {
          callbackProgress && callbackProgress(currentChunk, chunks, file)
          loadNext() // 继续切割下一块文件
        } else {
          params.file_name = file.name 
          params.use_scene = params.use_scene  // 使用场景，这是我们上传的时候记录的，以后好查询数据
          params.file_md5 = spark.end() // 前端计算好md5 丢给后端，我前面说过md5可以给后台做是否上传过，
          // 但是我这里在最后面才做了md5.end(), 因为我们后台没有做这样的校验，提个建议就是，在多出一个接口，传md5来判断当前文件是否上传过，有结束，返回url，没有走上传逻辑
          let res = await uploadComplete(params)
          callbackProgress && callbackProgress(currentChunk, chunks, file)
          // 文件上传成功
          callback && callback(params)
          // 初始化数据
          currentChunk = 0
          // 返回出去
          resolve()
        }
      }

      //处理单片文件的上传
      async function loadNext() {
        currentChunk++ // 更新当前上传的是第几块
        // 计算切割文件的开始索引和结束索引
        let start = (currentChunk - 1) * chunkSize
        let end = Math.min(start + chunkSize, file.size)
				// fileReader.readAsArrayBuffer 这个东西也是有点讲究的，有兴趣自己去研究fileReader
        // 切割文件并触发fileReader.onload
        fileReader.readAsArrayBuffer(blobSlice.call(file, start, end))
      }
    })
  }
}

// 分片的逻辑大概这些，其实还有很多地方需要完善的，苦于太忙了，这里捕获错误也是没有做的，希望你可以加上 😅😅
```

效果图

<img src="/Users/atoe/Desktop/blog/-/static/images/分片上传，以及限制上传数量.png" alt="结果图" style="zoom:30%;" />

